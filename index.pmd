% Algorithm Report 1: Build Order Scheduling
% Jon Craton

[![Build Status](https://travis-ci.org/jncraton/build-scheduler.svg?branch=master)](https://travis-ci.org/jncraton/build-scheduler) 
[![GitHub last commit](https://img.shields.io/github/last-commit/jncraton/build-scheduler.svg)](https://github.com/jncraton/build-scheduler)
![Codacy grade](https://img.shields.io/codacy/grade/c6b117c2fbdb499e91286969e2cbd137.svg)

Scheduling algoritms are important in many areas of Computer Science including CPU design, compiler design[1], and process scheduling. I will explore the task of applying scheduling to optimizing the ordering of dependent production tasks in the game StarCraft: Brood War.

This sort of planning problem has high computational complexity when searching for an optimal solution.[17] depending on how the problem is framed, a basic planning problem may have PSPACE-complete complexity, and the details of StarCraft make this problem even more complex. I will explore several heuristics to make this problem more tractable for practical and even real time applications.

StarCraft Overview
==================

StarCraft is a real-time strategy (RTS) game where players compete to build an economy and military with the goal of eliminating one another. A build order in StarCraft represents the order in which a player builds workers, units, and production structures in order to achieve certain goals as quickly as possible. This video provides a simple example of one basic build order:

<iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/w_SKXc22Pmg?rel=0&amp;showinfo=0&amp;start=23" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>

Let's define some StarCraft units:

```python
from collections import namedtuple

Unit = namedtuple('Unit', ['name','borrows','min','gas','supply','time','requires'])

Nexus = Unit('Nexus','Probe',400,0,-10,75,tuple())
Assimilator = Unit('Assimilator','Probe',100,0,0,25,tuple())
Pylon = Unit('Pylon','Probe',100,0,-8,19,tuple())
Gateway = Unit('Gateway','Probe',150,0,0,38,('Pylon',))
CyberneticsCore = Unit('CyberneticsCore','Probe',200,0,0,38,('Gateway',))
Observatory = Unit('Observatory','Probe',50,100,0,19,('RoboticsFacility',))
RoboticsFacility = Unit('RoboticsFacility','Probe',200,200,0,50,('CyberneticsCore',))
CitadelOfAdun = Unit('CitadelOfAdun','Probe',150,100,0,38,('CyberneticsCore',))
TemplarArchives = Unit('TemplarArchives','Probe',150,200,0,38,('CitadelOfAdun',))

Probe = Unit('Probe','Nexus',50,0,1,13,tuple())
Zealot = Unit('Zealot','Gateway',100,0,2,25,('Gateway',))
Dragoon = Unit('Dragoon','Gateway',125,50,2,32,('CyberneticsCore',))
DarkTemplar = Unit('DarkTemplar','Gateway',125,100,2,32,('TemplarArchives',))
HighTemplar = Unit('HighTemplar','Gateway',50,150,2,50,('TemplarArchives',))
Observer = Unit('Observer','RoboticsFacility',25,75,1,25,('Observatory',))

units = locals()
```

We also need a little math to calculate income rate based on the current units and structures we possess:

```python
def mining_rate(bases, assimilators, workers):
  """ Returns minerals/sec for a given number of bases, workers, and assimilators 

  https://liquipedia.net/starcraft/Mining

  Check that one base mineral rates scale properly:

  >>> mining_rate(1, 0, 9)
  (9.75, 0.0)
  >>> mining_rate(1, 0, 18)
  (18.0, 0.0)
  >>> mining_rate(1, 0, 27)
  (26.25, 0.0)
  >>> mining_rate(1, 0, 50)
  (26.25, 0.0)

  Check gas rates:

  >>> mining_rate(1, 1, 3)
  (0.0, 5.15)
  >>> mining_rate(1, 2, 6)
  (0.0, 10.3)

  Make sure we don't have any crosstalk between gas and mineral rates:

  >>> mining_rate(1, 1, 12)
  (9.75, 5.15)
  >>> mining_rate(1, 1, 21)
  (18.0, 5.15)
  >>> mining_rate(1, 1, 30)
  (26.25, 5.15)
  >>> mining_rate(1, 1, 50)
  (26.25, 5.15)
  """
  
  min_workers = workers - (assimilators * 3)

  min_workers_per_base = min_workers / bases
  min_per_base = min(min_workers_per_base, 9) * 65/60
  if min_workers_per_base > 9:
    min_per_base += min(min_workers_per_base - 9, 18) * 55/60
      
  return (min_per_base * bases, assimilators * (309 / 60))
```

CPU Scheduling Algorithms
=========================

I will begin by exploring this task using a CPU scheduling metaphor. Anything that can perform a task will be thought of as an Execution Unit[3]. One unique aspect of scheduling execution units in the case of Brood War is that our CPU is able to build new execution units on the fly.

We will first apply some traditional algorithms to handle the typical case where the number and type of execution units are fixed.

We'll create a class to wrap our execution units in to track what they are running and how long it will take to finish the work:

```python
def in_production(eus, unit):
  return [e for e in eus if e.running and e.running==unit]

def finished(eus, unit):
  return [e for e in eus if e.unit==unit]

def queued(tasks, unit):
  return [t for t in tasks if t==unit]

def owned(eus, tasks, unit):
  return finished(eus,unit) or queued(tasks,unit) or in_production(eus, unit)

class ExecutionUnit:
  def __init__(self, unit):
    self.unit = unit
    self.idle_at = 0
    self.running = None

  def complete(self):
    completed = self.running
    self.running = None
    self.idle_at = 0
  
    return completed

  def run(self, task, res, time):
    self.running = task
    self.idle_at = task.time + time

    res[0] -= task.min
    res[1] -= task.gas
    if task.supply > 0: # Otherwise we adjust supply on completion
      res[2] -= task.supply

  def can_run(self, eus, task, res):
    if task.min > res[0] or task.gas > res[1] or task.supply > res[2]: return False

    for r in task.requires:
      if not finished(eus, units[r]): return False
  
    return not self.running and (task.borrows == None or self.unit.name==task.borrows)
```

Now let's add an `simulate` function to plan a list of tasks. This function takes a list of execution units, a list of tasks to complete, and a scheduler function to decide what to run each frame.

```python
from collections import Counter

def simulate(tasks, scheduler, units=[Nexus] + [Probe] * 4, debug=False, debug_completed=False):
  """
  Simulates a build for a list of tasks using a supplied scheduler
  
  >>> simulate([Pylon, Gateway], lambda *x: None)
  85
  >>> simulate([Probe] * 4 + [Pylon, Gateway, Zealot], lambda *x: None)
  127
  >>> simulate([Probe] * 4 + [Pylon, Gateway, Zealot, Nexus, Zealot], lambda *x: None)
  196
  >>> simulate([Probe] * 4 + [Pylon, Gateway, Zealot, Nexus, Zealot], lambda eus,tasks,*_: tasks.pop())
  Traceback (most recent call last):
      ...
  Exception: Units were dropped from the build order
  """

  time = -1
  res = [50,0,6]
  expected_units = Counter([e.name for e in tasks[:] + units[:]])

  eus = [ExecutionUnit(unit) for unit in units]

  if debug:
    print("Creating optimized build order for:") 
    for u in expected_units:
      print("  %s: %d" % (u, expected_units[u]))
    print("\nBuild order:")

  def message(msg):
    if debug:
      print('%3ds %4d ore %4d gas %3d supply   %s' % (time, res[0], res[1], res[2], msg))

  for time in range(0,60000):
    for eu in eus:
      if eu.running and eu.idle_at <= time:
        if debug_completed:
          message("Completed %s." % eu.running.name)
        if eu.running.supply < 0:
          res[2] -= eu.running.supply
        eus.append(ExecutionUnit(eu.complete()))

    scheduler(eus, tasks, time, res)

    # Begin running the next task if possible
    if tasks:
      for eu in eus:
        if eu.can_run(eus,tasks[0], res):
          message("Started %s" % tasks[0].name)
          eu.run(tasks[0], res, time)
          tasks.pop(0)
          break

    if not [t for t in tasks if t not in [Probe,Pylon]] and not [idle for idle in eus if not idle.running in [None,Probe,Pylon]]: break

    peons = len([t for t in eus if t.unit == Probe])
    bases = len([t for t in eus if t.unit == Nexus])
    gases = len([t for t in eus if t.unit == Assimilator])
    
    res[0] += mining_rate(bases, gases, peons)[0]
    res[1] += mining_rate(bases, gases, peons)[1]

  unit_count = Counter([e.unit.name for e in eus])

  # Confirm that we completed at least as many units as requested
  if not all(n <= unit_count[k] for k, n in expected_units.items()):
    raise Exception("Units were dropped from the build order")

  if debug:
    print("\nFinal units:")
    for u in unit_count:
      print("  %s: %d" % (u, unit_count[u]))
    print("\nTotal makespan: %ds" % time)

  return time
```

FIFO (In-order execution)
=========================

One of the simplest scheduling algorithms that we can implement is a simple FIFO system:

```python
def fifo(eus, tasks, time, res):
  """
  Assign tasks to execution units in the order they were supplied.

  This is a noop.
  """

  return

TASKS = [Probe] * 4 + [Pylon] + [Probe] * 4 + [Gateway, Zealot, Zealot]

simulate(TASKS[:], fifo, debug=True)
```

One weakness of FIFO is that it fails to allow future tasks to run in paralell.

Correctness and performance
---------------------------

This "algorithm" is obviously correct as all tasks are completed. It takes no time to simulate as it doesn't do anything.

Reordering (out-of-order execution)
===================================

We'll now implement a simple algorithm to allow future operations to be scheduled if the next operation is blocked. The CPU metaphor is not perfect here, but this borrows the concept of dependency handling and a reservation station from Tomasulo's algorithm. [4][9]

An example of the overall architecture and usage of this algorithm is moder superscalar CPUs can be seen in this diagram of the Core 2 microarchitecture[21]:

![](media/intel-core2-arch.png)

This is the basic algorithm we'll implement:

    each game frame:
      if we don't have an instruction in the reservation station:
        for task in queue:
          if task is runnable, add it to the reservation station

Here's the implementation:

```python
def reorder(eus, tasks, time, res):
  """
  Reorders upcoming tasks if the next task is not runnable. This implements the first step of a selection sort to move the first task that can be run to the front of the task queue.

  The algorithm used here is to simply put the first runnable task as the start of the task queue or do nothing
  """

  for (i, task) in enumerate(tasks):
    for eu in eus:
      if eu.can_run(eus,task, res):
        tasks.insert(0, tasks.pop(i))
        return

simulate(TASKS[:], reorder, debug=True)
```

We were able to remove 23s of stalling on this simple task by simply using reordering.

Correctness
-----------

This algorithm is correct because it still includes all tasks. It returns provably suboptimal makespans in most cases as we will show soon. It adheres to the required dependencies, so tasks are not schduled until they are legally able to be simulated.

Performance
-----------

In the worst case, this algorithm requires a linear scan of all current units each game frame. Because the number of units steadily increases, we see that this algorithm has O(n²) complexity.

Routine task automation heuristics
==================================

Worker production
-----------------

In RTS games, a certain economic size (worker count, base count, etc) is typically a means to an end rather than a goal in itself. For this reason, I have created a scheduler that expects to not be told when to build workers or bases and instead automatically injects these as needed.

For this simple implementation, the scheduler simply builds a worker if it is able to do so and commands fewer than 30 of them. It does not create additional bases.

```python
def automin(eus, tasks, time, res):
  for eu in eus:
    if eu.can_run(eus,Probe, res) and len([e for e in eus if e.unit == Probe]) <= 30:
      tasks.insert(0, Probe)
      return True

def reorder_automin(eus, tasks, time, res):
  reorder(eus, tasks, time, res)
  automin(eus, tasks, time, res)

simulate([Pylon, Gateway, Zealot, Zealot], reorder_automin, debug=True)
```

Gas management
--------------

Brood War has a secondary resource called Vespene Gas (typically just gas) that can be obtained some point after the start of the game, but not immediately. Gas timing is a very important part of build order design, and it is thus far not handle by our simple algorithm.

We will update our scheduler to build a gas mining facility (Assimilator) once it is blocked by a unit that requires gas and has none.

```python
def autogas(eus, tasks, time, res):
  if not tasks: return
  
  if tasks[0].gas > 0 and not owned(eus, tasks, Assimilator):
    for eu in eus:
      if eu.can_run(eus,Assimilator, res):
        tasks.insert(0, Assimilator)
        return True

def reorder_autogas(eus, tasks, time, res):
  reorder(eus, tasks, time, res)
  if autogas(eus, tasks, time, res): return
  if automin(eus, tasks, time, res): return

simulate([Pylon, Gateway, Zealot, Zealot, Pylon, CyberneticsCore, Dragoon], reorder_autogas, debug=True)
```

This handles ensuring that we don't get blocked on units that require gas, but it is a very weak planning method.

Supply management
-----------------

Each unit created in Brood War borrows a fixed amount of "supply" from the players total supply. Supply is created by creating Pylons. We can automatically manage supply in the same way as we just did for gas.

```python
def autosupply(eus, tasks, time, res):
  if not tasks: return

  if (tasks[0].supply > res[2] or (time > 120 and res[2] < 8)) and not in_production(eus, Pylon):
    for eu in eus:
      if eu.can_run(eus, Pylon, res):
        tasks.insert(0, Pylon)
        return True

def supply_gas(eus, tasks, time, res):
  if not tasks: return
  if autogas(eus, tasks, time, res): return
  if autosupply(eus, tasks, time, res): return

def supply_gas_min(eus, tasks, time, res):
  if not tasks: return
  if supply_gas(eus, tasks, time, res): return
  if automin(eus, tasks, time, res): return

def reorder_autosupply(eus, tasks, time, res):
  reorder(eus, tasks, time, res)
  supply_gas_min(eus, tasks, time, res)

simulate([Pylon, Gateway, Zealot, Zealot, CyberneticsCore, Dragoon], reorder_autosupply, debug=True)
```

Now we should never be supply blocked. We can also see that this algorithm completes the requested tasks in the same amount of time as the above, but has a stronger economic position.

Correctness
-----------

Given that these few additions never remove tasks from the schedule, they will return valid schedules.

Performance
-----------

These simple constant time heuristics add nothing the the algorithm in terms of big-O complexity, but they do improve the performance of the final makespan significantly significantly.

Topological Sorting 
===================

Let's consider a more complex production problem that includes multiple branches and explore how to optimize it. Let's try to build an Observer and a Dark Templar as quickly as possible.

```python
tasks = [Pylon, Gateway, CyberneticsCore, 
         RoboticsFacility, Observatory, Observer, 
         CitadelOfAdun, TemplarArchives, DarkTemplar
        ]

simulate(tasks[:], reorder_autosupply, debug=True)
```

We need to add better planning to handle complicated dependencies. The first step to handling dependencies in a sensible way is to convert our operations into a directed acyclic graphs (DAG).

Directed Acyclic Graph
----------------------

```python
import math
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

class Node:
  def __init__(self, task):
    self.task = task

  def __str__(self):
    return self.task.name

def get_dag(tasks):
  dag = nx.DiGraph()

  for task in set(tasks):
    dag.add_node(Node(task))

  for n in dag.nodes(data=True):
    n[1]['count'] = len([t for t in tasks if t==n[0].task])
  
  for n1 in dag.nodes:
    for n2 in dag.nodes:
      weight=n2.task.time
      
      if n2.task.borrows == n1.task.name:
        weight = n2.task.time * math.ceil(dag.node[n2]['count'] / dag.node[n1]['count'])
        dag.add_edge(n1,n2,weight=weight)
        
      for r in n2.task.requires:
        if r == n1.task.name:
          dag.add_edge(n1,n2,weight=weight)

  return dag

def draw_dag(dag):
  pos=nx.spectral_layout(dag)
  plt.figure(1,figsize=(6,6)) 
  nx.draw(dag,pos, with_labels=True, node_size=60,font_size=12)
  labels = nx.get_edge_attributes(dag,'weight')
  nx.draw_networkx_edge_labels(dag,pos,edge_labels=labels, font_size=12)

draw_dag(get_dag(tasks))
```

This DAG gives us the data sctructure we need to begin reasoning about how much overall makespan duration depends on a particular task and optimize our scheduling accordingly.

Note that each task type is included only once in the DAG. The path weights are used to track total time to complete a set of identical task. For example, if the current schedule includes building 12 Zealots from 2 Gateways, the DAG will include a Gateway node pointing to a Zealot node with a weight of `(12 Zealot / 2 Gateways) * Zealot build time`. This optimization makes it possible to reason easily about paralell tasks by path length through the graph.

Kahn's Algorithm
----------------

Next, we need to process our DAG into a list that is sorted topologically.[6] This allows us to determine what we are allowed to do next.

We'll simply apply Kahn's algorithm to do the sorting[10]. Here's the psuedocode from Wikipedia [6]:

    L ← Empty list that will contain the sorted elements
    S ← Set of all nodes with no incoming edge
    while S is non-empty do
        remove a node n from S
        add n to tail of L
        for each node m with an edge e from n to m do
            remove edge e from the graph
            if m has no other incoming edges then
                insert m into S
    if graph has edges then
        return error (graph has at least one cycle)
    else 
        return L (a topologically sorted order)

Now we simply sort our DAG topologically:

```python
[str(n) for n in nx.topological_sort(get_dag(tasks))]
```

Performance
-----------

As we can see from examining this algorithm, it will handle each vertex exactly once and handle each edge once, so the overall performance will simple be O(n), where n is the number of vertices and edges in the graph. However, because we need to run this algorithm once for every frame in our makespan, the total performance is O(n²). This performance asseassumes that we do the work to store and maintain a DAG throughout the process.

Correctness
-----------

This algorithm is correct as items are only added to the sorted list when they have no incoming edges. By definition, this means that L is sorted topologically. In the case where not all vertices are included in L, we can show that this is due to a cycle in our DAG and topological sorting is impossible.

Critical Path
=============

Our scheduling system up to this point does not include any advanced planning and simply tries to grab the first available piece of work. What we would like to do is find the critical path[13] through our DAG and begin working on it first. Generally, we need to solve the longest path problem[14] for our DAG.

> The critical path method for scheduling a set of activities involves the construction of a directed acyclic graph in which the vertices represent project milestones and the edges represent activities that must be performed after one milestone and before another; each edge is weighted by an estimate of the amount of time the corresponding activity will take to complete. In such a graph, the longest path from the first milestone to the last one is the critical path, which describes the total time for completing the project. [14]

The longest path problem for a general graph is NP-hard, but the variant needed to DAGs is not particularly interesting and linear time. Here's the basic idea:

> 1. Find a topological ordering of the given DAG.
> 2. For each vertex v of the DAG, in the topological ordering, compute the length of the longest path ending at v by looking at its incoming neighbors and adding one to the maximum length recorded for those neighbors. If v has no incoming neighbors, set the length of the longest path ending at v to zero. In either case, record this number so that later steps of the algorithm can access it.
> 3. Once this has been done, the longest path in the whole DAG may be obtained by starting at the vertex v with the largest recorded value, then repeatedly stepping backwards to its incoming neighbor with the largest recorded value, and reversing the sequence of vertices found in this way. [14]

For our purposes, we will be using a weighted DAG. The algorithm is essentially the same except that path weights are used instead of simply incrementing by 1 for each path.

Here's the result of applying this critical path algorithm to our DAG:

```python
[str(n) for n in nx.dag_longest_path(get_dag(tasks))]
```

Let's now build a scheduler that uses the critical path to decide what to run next.

```python
def critical_path(eus, tasks, time, res):
  if not tasks: return

  if autosupply(eus, tasks, time, res): return
  if automin(eus, tasks, time, res): return
  if autogas(eus, tasks, time, res): return

  new_start = nx.dag_longest_path(get_dag(tasks))[0]
  tasks.remove(new_start.task)
  tasks.insert(0, new_start.task)

simulate(tasks[:], critical_path, debug=True)
```

Note that the scheduler must recalculate the critical path on each run as the first element of the critical path may sometimes not be the second element from the previous run. This is due to the fact that there may be multiple paralell paths that need to be simulated in slices to optimize time efficiency.

Performance
-----------

This scheduler ends up with O(n²) complexity with the number of steps in the build. It requires linear time to determine the critcal path, but this path must be calculated again for every step, so we end up with O(n²) time.

Correctness
-----------

Due to the topological sorting, this algorithm is obviously correct. Starting with paths of length zero, it incrementally adds length as it iterates in topological order. Every vertex is visited and is assigned a correct weight. This algorithm is correct in that it completes every requested task, but it is still not optimal.

Parallelizing Tasks
===================

The next thing that we need to add to our scheduler is the concept of bottleneck removal. Consider the following task:

```python
zealots = [Pylon,Gateway] + [Zealot]*12
simulate(zealots[:], critical_path, debug=True)
```

And the simple DAG:

```python
draw_dag(get_dag(zealots))
```

Our previous algorithm misses a key issue. While the gateway allows zealots to be built, it also creates a bottleneck. Right now, zealots are built individually over a long period of time. In Brood War, we have the option to invest in any number of gateways, so let's update our algorithm to handle explore that possibility.

Let's try adding some tests to see if we can improve our longest path. Here's the current longest path:

```python
nx.dag_longest_path_length(get_dag(zealots))
```

And now with an extra gateway:

```python
two_gates = get_dag(zealots + [Gateway])
draw_dag(two_gates)
nx.dag_longest_path_length(two_gates)
```

This is almost twice as fast. Let's see if we can create an algorithm to automatically parallelize these hot paths in a scheduler.

```python
def parallelize(eus, tasks, time, res):
  if not tasks: return

  final = nx.dag_longest_path(get_dag(tasks))[-1]
  bottleneck = final.task.borrows

  completed = [e.unit for e in eus[5:] if e.unit.borrows == 'Probe'] # Exclude starting units

  current = nx.dag_longest_path_length(get_dag(completed + tasks))
  new = nx.dag_longest_path_length(get_dag(completed + tasks + [units[bottleneck]]))

  if new < current:
    for eu in eus:
      if eu.can_run(eus, units[bottleneck], res):
        tasks.insert(0, units[bottleneck])
        return True

  critical_path(eus, tasks, time, res)
  
simulate(zealots[:], parallelize, debug=True)
```

Performance Comparison
======================

Let's create a fairly complex problem to schedule and then measure the performance of each algorithm both in terms of compuational time and in terms of total makespan.

Makespan vs tasks
-----------------

```python, results='hidden'
tasks = [Pylon,Gateway,
  Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, Zealot, 
  CyberneticsCore,
  Dragoon, Dragoon, Dragoon, Dragoon, Dragoon, Dragoon,Dragoon, Dragoon, Dragoon,Dragoon, Dragoon, Dragoon,
  RoboticsFacility,Observatory,
  Observer, Observer,
  CitadelOfAdun,TemplarArchives,
  HighTemplar,HighTemplar,HighTemplar,HighTemplar,
  DarkTemplar,DarkTemplar,DarkTemplar,DarkTemplar,
]

algs = ['supply_gas', 'supply_gas_min', 'reorder_autosupply', 'critical_path', 'parallelize']

for alg in algs:
  makespan = []
  for i in range(0, len(tasks), 5):
    makespan.append((i, simulate(tasks[0:i], locals()[alg])))

  plt.plot([m[0] for m in makespan], [m[1] for m in makespan], label=alg)

plt.xlabel('Number of tasks')
plt.ylabel('Makespan (s)')
plt.title('Makespan vs number of tasks')
plt.legend()
```

Interestingly, we see that for this task, naive critical path searching performs worse than simple reordering in terms of total makespan. This is due to the fact that it over focuses on developing its longest tech route instead of squeezing in units where it can.

Scheduler time vs tasks
-----------------------

```python,results="hidden"
import time

for alg in algs:
  runtime = []
  for i in range(0, len(tasks), 2):
    start = time.process_time()
    for j in range(0,100):
      locals()[alg]([ExecutionUnit(u) for u in ([Probe] * 4 + [Pylon, Gateway, Zealot])],tasks[0:i],0,[50,0,6])
    runtime.append((i, (time.process_time() - start) * 10))

  plt.plot([m[0] for m in runtime], [m[1] for m in runtime], label=alg)

plt.xlabel('Number of tasks')
plt.ylabel('Runtime (ms)')
plt.title('Scheduler single runtime vs tasks')
plt.legend()
```

```python,results="hidden"
for alg in algs:
  runtime = []
  for i in range(0, len(tasks), 5):
    start = time.process_time()
    simulate(tasks[0:i], locals()[alg])
    runtime.append((i, (time.process_time() - start) * 10))

  plt.plot([m[0] for m in runtime], [m[1] for m in runtime], label=alg)

plt.xlabel('Number of tasks')
plt.ylabel('Runtime (ms)')
plt.title('Total runtime vs tasks')
plt.legend()
```

Other Algorithms
================

Brute Force
-----------

The search space for a build order is actually very large due to it's exponential nature. Brute force for most ways of framing this problem is superpolynomial.

Depth-First Branch and Bound
----------------------------

Depth-first branch and bound as suggested by Churchill[16] will produce an optimal makespan. This algorithm will require superpolynomial time as the search space is superpolynomial, but it has the advantage of returning at any point with a working schedule. This feature is useful for real-time applications. 

Genetic Algorithms
------------------

Genetic algorithms are a common way to solve this problem. While not elimating the high computational complexity of the complete solution, genetic algorithms allow us to hopefully asymtotically approach the ideal solution with respect to computing time. Genetic algorithms have been applied in this field and created nice practical results.[19] One notable result was the discovery of the 7 Roach Rush in 2010.

Future Work
===========

There are certainly more complex algorithms that could be adapted to this task.

The algorithms here also each assume a 1-base economy, which is an extreme simplification and makes these results very poor when compared against long build orders simulated by professional players. Adding a more complete economic model should greatly improved total makespan and add very little computational complexity.

References
==========

1. http://www.cl.cam.ac.uk/teaching/2005/OptComp/slides/lecture14.pdf
2. https://en.wikipedia.org/wiki/Scheduling_(computing)
3. https://en.wikipedia.org/wiki/Execution_unit
4. https://www.cc.gatech.edu/fac/milos/Teaching/CS6290F07/4_Tomasulo.pdf
5. https://www.kth.se/social/upload/62/microprocessor_design.pdf
6. https://en.wikipedia.org/wiki/Topological_sorting
7. https://en.wikipedia.org/wiki/Coffman%E2%80%93Graham_algorithm
8. https://en.wikipedia.org/wiki/Job_shop_scheduling
9. https://en.wikipedia.org/wiki/Tomasulo_algorithm
10. Kahn, Arthur B. (1962), "Topological sorting of large networks", Communications of the ACM, 5 (11): 558–562, doi:10.1145/368996.369025.
11. http://www.cs.mun.ca/~dchurchill/pdf/aiide11-bo.pdf
12. http://richoux.fr/publications/ecgg15_chapter-rts_ai.pdf
13. https://en.wikipedia.org/wiki/Critical_path_method
14. https://en.wikipedia.org/wiki/Longest_path_problem
15. http://www.cs.nthu.edu.tw/~wkhon/ds/ds11/lecture/lecture12.pdf
16. Churchill, David, and Michael Buro. "Build Order Optimization in StarCraft." In AIIDE, pp. 14-19. 2011.
17. Bylander, Tom. "Complexity Results for Planning." In IJCAI, vol. 10, pp. 274-279. 1991.
18. Chan, Hei, Alan Fern, Soumya Ray, Nick Wilson, and Chris Ventura. "Extending online planning for resource production in real-time strategy games with search." In Workshop on Planning in Games, ICAPS, vol. 2007. 2007.
19. http://lbrandy.com/blog/2010/11/using-genetic-algorithms-to-find-starcraft-2-build-orders/
20. https://www.youtube.com/watch?v=-8IycRLEBok
21. https://en.wikipedia.org/wiki/Microarchitecture
22. https://liquipedia.net/starcraft/Main_Page
23. https://liquipedia.net/starcraft/Mining
